{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"}, {"metadata": {}, "cell_type": "markdown", "source": "# Tutorial on generating an explanation for an image-based model on Watson OpenScale"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook includes steps for creating an image-based watson-machine-learning model, creating a subscription, configuring explainability, and finally generating an explanation for a transaction."}, {"metadata": {}, "cell_type": "markdown", "source": "### Contents\n- [1. Setup](#setup)\n- [2. Creating and deploying an image-based model](#deployment)\n- [3. Subscriptions](#subscription)\n- [4. Explainability](#explainability)"}, {"metadata": {}, "cell_type": "markdown", "source": "**Note**: If using Watson Studio, try running the notebook on atleast 'Default Python 3.5 XS' version for faster results."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n## 1. Setup\n\n### 1.1 Install Watson OpenScale and WML packages"}, {"metadata": {}, "cell_type": "code", "source": "!pip install --upgrade ibm-ai-openscale --no-cache | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install --upgrade watson-machine-learning-client --no-cache | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Note: Restart the kernel to assure the new libraries are being used."}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.2 Configure credentials"}, {"metadata": {}, "cell_type": "markdown", "source": "Get the IBM Cloud `apikey` by going to the [IBM Cloud API Keys console](https://cloud.ibm.com/iam/apikeys) and clicking \"Create an IBM Cloud API Key\", copy & paste it in the cell below.\n\nOne can obtain the Watson OpenScale `instance_id` (guid) by accessing the [IBM Cloud resource list](https://cloud.ibm.com/resources), clicking on `Services` and clicking anywhere on the Watson OpenScale service tile except for the service link and then checking the popping sidebar on the right."}, {"metadata": {}, "cell_type": "code", "source": "AIOS_CREDENTIALS = {\n    \"instance_guid\": \"***************\",\n    \"apikey\": \"***************\", \n    \"url\": \"https://api.aiopenscale.cloud.ibm.com\"\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Generate or fetch the WML credentials by clicking on `Credentials` in the sidebar of the provisioned WML page and paste it below."}, {"metadata": {}, "cell_type": "code", "source": "WML_CREDENTIALS = {\n  \"apikey\": \"***************\",\n  \"iam_apikey_description\": \"***************\",\n  \"iam_apikey_name\": \"***************\",\n  \"iam_role_crn\": \"***************\",\n  \"iam_serviceid_crn\": \"***************\",\n  \"instance_id\": \"***************\",\n  \"url\": \"***************\"\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"deployment\"></a>\n## 2. Creating and deploying an image-based model"}, {"metadata": {}, "cell_type": "markdown", "source": "The dataset used is MNIST dataset of handwritten digits. It consists of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. More information about the dataset can be found here: https://keras.io/datasets/#mnist-database-of-handwritten-digits\n\nNote: Keras and TensorFlow versions supported by WML are: Keras 2.1.6 with TensorFlow 1.13 backend and Keras 2.2.4 with TensorFlow 1.14 backend. The latter combination is used in this notebook."}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1 Creating a model"}, {"metadata": {}, "cell_type": "code", "source": "!pip install keras==2.2.4\n!pip install tensorflow==1.6.0\n!pip install keras_sequential_ascii\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras_sequential_ascii import sequential_model_to_ascii_printout\nfrom keras import backend as keras_backend\nprint(keras.__version__)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "batch_size = 128\nnum_classes = 10\nepochs = 5", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif keras_backend.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Define Model\n\ndef base_model():\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer=keras.optimizers.Adadelta(),\n                  metrics=['accuracy'])\n    return model", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "cnn_n = base_model()\ncnn_n.summary()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Vizualizing model structure\nsequential_model_to_ascii_printout(cnn_n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Fit model\nprint(y_train.shape)\ncnn = cnn_n.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "scores = cnn_n.evaluate(x_test, y_test, verbose=0)\nprint(scores)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2 Storing the model"}, {"metadata": {}, "cell_type": "code", "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n\nwml_client = WatsonMachineLearningAPIClient(WML_CREDENTIALS)\ncnn_n.save(\"mnist_cnn.h5\")\n!rm mnist_cnn.tar*\n!tar -czvf mnist_cnn.tar.gz mnist_cnn.h5", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!rm mnist_cnn.h5", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_name = \"MNIST Model\"\n\n# Update the FRAMEWORK_VERSION below depending on the tensorflow version used\nmodel_meta = {\n    wml_client.repository.ModelMetaNames.NAME: model_name,\n    wml_client.repository.ModelMetaNames.DESCRIPTION: \"MNIST model\",\n    wml_client.repository.ModelMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n    wml_client.repository.ModelMetaNames.FRAMEWORK_VERSION: \"1.15\",\n    wml_client.repository.ModelMetaNames.FRAMEWORK_LIBRARIES: [\n         {\"name\": \"keras\", \"version\": \"2.2.4\"}\n    ]\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "published_model_details = wml_client.repository.store_model(model='mnist_cnn.tar.gz', meta_props=model_meta)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_uid = wml_client.repository.get_model_uid(published_model_details)\nmodel_uid", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.3 Deploying the model"}, {"metadata": {}, "cell_type": "code", "source": "deployment= wml_client.deployments.create(name= model_name + \" Deployment\", model_uid=model_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "scoring_url = wml_client.deployments.get_scoring_url(deployment)\nprint(scoring_url)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3. Subscriptions"}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1 Configuring OpenScale"}, {"metadata": {}, "cell_type": "code", "source": "from ibm_ai_openscale import APIClient\nfrom ibm_ai_openscale.engines import WatsonMachineLearningAsset\n\naios_client = APIClient(AIOS_CREDENTIALS)\naios_client.version", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.2 Subscribe the asset"}, {"metadata": {}, "cell_type": "code", "source": "from ibm_ai_openscale.supporting_classes import *\n\nsubscription = aios_client.data_mart.subscriptions.add(WatsonMachineLearningAsset(\n    model_uid,\n    problem_type=ProblemType.MULTICLASS_CLASSIFICATION,\n    input_data_type=InputDataType.UNSTRUCTURED_IMAGE,\n    probability_column='probability'\n))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "aios_client.data_mart.subscriptions.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscription.get_details()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.3 Score the model and get transaction-id"}, {"metadata": {}, "cell_type": "code", "source": "!pip install numpy\n!pip install matplotlib\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline \nimg = np.array(x_test[999], dtype='float')\npixels = img.reshape((28, 28))\nplt.imshow(pixels, cmap='gray')\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "scoring_data = {'values': [x_test[999].tolist()]}\npredictions = wml_client.deployments.score(scoring_url, scoring_data)\nprint(predictions)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Note: Please wait for a few seconds before running the cell below."}, {"metadata": {}, "cell_type": "code", "source": "transaction_id = subscription.payload_logging.get_table_content().scoring_id[0]\nprint(transaction_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"explainability\"></a>\n## 4. Explainability"}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.1 Configure Explainability"}, {"metadata": {}, "cell_type": "code", "source": "subscription.explainability.enable()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscription.explainability.get_details()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.2 Get explanation for the transaction"}, {"metadata": {}, "cell_type": "code", "source": "explanation = subscription.explainability.run(transaction_id, background_mode=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "explanation", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### The explanation images can be obtained using the cells below"}, {"metadata": {}, "cell_type": "code", "source": "!pip install Pillow\nfrom PIL import Image\nimport base64\nimport io\n\nimg = explanation[\"entity\"][\"predictions\"][0][\"explanation\"][0][\"full_image\"]\nimg_data = base64.b64decode(img)\nImage.open(io.BytesIO(img_data))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "img = explanation[\"entity\"][\"predictions\"][1][\"explanation\"][0][\"full_image\"]\nimg_data = base64.b64decode(img)\nImage.open(io.BytesIO(img_data))", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}